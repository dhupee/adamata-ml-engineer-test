{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Adamata Test Notebook","metadata":{"_uuid":"fe212db2-3c63-409d-ae0f-42d99735786d","_cell_guid":"379fcd57-1ee5-45a2-a0c8-9af03b8de390","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Get python version","metadata":{"_uuid":"6db3093e-2a87-4a8a-a269-6e427f7f105d","_cell_guid":"ec8dac19-6167-47d9-b2e2-63b1ce1931b9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# This version will decide the version I will use\n!python -V","metadata":{"_uuid":"6d155d66-5ed4-49a0-af8c-1f730b15fd7a","_cell_guid":"830726ac-8770-412d-843a-5411c1932381","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Understanding the datasets a little bit\n\nwe're going to download the dataset given from google drive, we just going to use gdown and unzip to download then unzip it.\n\nso...we got 12 images with respective label, and turns out the label is all the same despite the image is different.\n\n```txt\nclass_id center_x center_y width height\n```\n\nif we see from YOLO annotation format, it looks that way, the label is all similar.","metadata":{"_uuid":"6547fdca-f2a6-4960-b8c1-8bd7e88dfecb","_cell_guid":"ff618bf0-779b-4090-a66d-36a9f773218b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install gdown","metadata":{"_uuid":"1341a5a1-d267-420c-b431-37d1fc016b2e","_cell_guid":"e934007c-2079-424a-aad5-20bf4459d323","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##","metadata":{"_uuid":"8bd68874-4263-43b7-aab3-7a8e84e18ff3","_cell_guid":"3121a3ec-2c36-4f25-8ee2-03d01f822cee","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"##","metadata":{"_uuid":"bf4a3143-d48b-4937-bd16-e0a274cb2056","_cell_guid":"5c14c577-f524-461b-8b2b-d911c2e8d85b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# download and extract given dataset\ndataset_url = \"https://drive.google.com/file/d/1vbMFTe2E5OHp2h5o_YL-RUoIFKxsrZWT/view?usp=sharing\"\ndataset_output_path = \"/kaggle/working/sample.zip\"\n!gdown --fuzzy {dataset_url} -O {dataset_output_path}\n!unzip {dataset_output_path}","metadata":{"_uuid":"52ff8f77-0552-4e12-bdaf-6fff461b7850","_cell_guid":"e82da30f-ea59-4e34-ae5a-db72d2ddd0be","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_directory = \"/kaggle/working/sample\"\n\nimport os\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef display_images_from_directory(directory_path):\n    \"\"\"\n    Opens a directory, identifies image files, and displays them in a grid.\n\n    Args:\n        directory_path (str): The path to the directory containing images.\n    \"\"\"\n    image_files = []\n    # Iterate through files in the specified directory\n    for filename in os.listdir(directory_path):\n        # Check if the file is an image (you can extend this list)\n        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n            image_files.append(os.path.join(directory_path, filename))\n\n    if not image_files:\n        print(f\"No image files found in '{directory_path}'\")\n        return\n\n    # Determine grid size for displaying images\n    num_images = len(image_files)\n    cols = int(num_images**0.5)  # Approximate square grid\n    rows = (num_images + cols - 1) // cols # Calculate rows to fit all images\n\n    plt.figure(figsize=(10, 10)) # Adjust figure size as needed\n\n    for i, image_path in enumerate(image_files):\n        try:\n            img = Image.open(image_path)\n            plt.subplot(rows, cols, i + 1)\n            plt.imshow(img)\n            plt.title(os.path.basename(image_path)) # Display filename as title\n            plt.axis('off') # Hide axes for cleaner display\n        except Exception as e:\n            print(f\"Error opening or displaying '{image_path}': {e}\")\n\n    plt.tight_layout() # Adjust subplot parameters for a tight layout\n    plt.show()\n\n# Example usage:\n# Replace 'path/to/your/images' with the actual path to your directory\ndisplay_images_from_directory(dataset_directory)","metadata":{"_uuid":"b31886df-9532-45d3-bede-581c2fa56147","_cell_guid":"c2fdf768-a4f3-46b3-8c96-03ccdc759743","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def cat_txt_files_in_directory(directory_path):\n    \"\"\"\n    Opens a directory and prints the content of all .txt files within it.\n\n    Args:\n        directory_path (str): The path to the directory to process.\n    \"\"\"\n    path_obj = Path(directory_path)\n\n    if not path_obj.is_dir():\n        print(f\"Error: '{directory_path}' is not a valid directory.\")\n        return\n\n    print(f\"Contents of .txt files in '{directory_path}':\\n\")\n    found_txt_files = False\n\n    for item in path_obj.iterdir():\n        if item.is_file() and item.suffix == '.txt':\n            found_txt_files = True\n            print(f\"--- {item.name} ---\")\n            try:\n                with open(item, 'r', encoding='utf-8') as f:\n                    print(f.read())\n                print() # Add a newline for better separation between files\n            except Exception as e:\n                print(f\"Error reading '{item.name}': {e}\")\n    \n    if not found_txt_files:\n        print(\"No .txt files found in this directory.\")\n\ncat_txt_files_in_directory(dataset_directory)","metadata":{"_uuid":"8c887474-5810-4790-9bcd-1cf3e9f04a47","_cell_guid":"4fe53ffb-703a-46d7-af41-d517e1f04151","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Try plot the dataset with the bounding boxes\n\nso the dataset is bunch of image with yolo annotated bounding boxes\n\nlets try to plot it alongside the bounding boxes","metadata":{"_uuid":"c11b0157-ab8a-4de8-b1ca-2bd0deb37736","_cell_guid":"410b2d6c-aabe-42f3-a6e7-66df9d8c4ff2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import os\nimport cv2\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\n\ndef display_images_with_bboxes(directory_path):\n    \"\"\"\n    Display images from a directory with YOLO format bounding boxes.\n    \n    Args:\n        directory_path (str): Path to the directory containing jpg and txt files\n    \"\"\"\n    dir_path = Path(directory_path)\n    \n    # Get all jpg files\n    jpg_files = list(dir_path.glob(\"*.jpg\"))\n    \n    if not jpg_files:\n        print(f\"No JPG files found in {directory_path}\")\n        return\n    \n    # Set up the plot\n    fig = plt.figure(figsize=(15, 10))\n    \n    for i, jpg_path in enumerate(jpg_files):\n        # Find corresponding txt file\n        txt_path = jpg_path.with_suffix('.txt')\n        \n        if not txt_path.exists():\n            print(f\"Warning: No corresponding txt file for {jpg_path.name}\")\n            continue\n            \n        # Read image\n        image = cv2.imread(str(jpg_path))\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        img_height, img_width = image.shape[:2]\n        \n        # Read bounding boxes from txt file\n        with open(txt_path, 'r') as f:\n            lines = f.readlines()\n        \n        # Draw bounding boxes\n        for line in lines:\n            data = line.strip().split()\n            if len(data) >= 5:  # class_id, x_center, y_center, width, height\n                class_id = int(data[0])\n                x_center = float(data[1]) * img_width\n                y_center = float(data[2]) * img_height\n                width = float(data[3]) * img_width\n                height = float(data[4]) * img_height\n                \n                # Calculate coordinates\n                x1 = int(x_center - width/2)\n                y1 = int(y_center - height/2)\n                x2 = int(x_center + width/2)\n                y2 = int(y_center + height/2)\n                \n                # Draw rectangle\n                cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n                \n                # Add label with class ID\n                label = f'Class {class_id}'\n                label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)[0]\n                \n                # Draw label background\n                cv2.rectangle(image, (x1, y1 - label_size[1] - 5), \n                             (x1 + label_size[0], y1), (255, 0, 0), -1)\n                \n                # Draw label text\n                cv2.putText(image, label, (x1, y1 - 5), \n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n        \n        # Display image\n        plt.subplot(3, 4, i+1)  # Adjust grid size as needed\n        plt.imshow(image)\n        plt.title(jpg_path.name)\n        plt.axis('off')\n        \n        # Stop if we've displayed 12 images (adjust as needed)\n        if i + 1 >= 12:\n            break\n    \n    plt.tight_layout()\n    plt.show()\n\n# Usage example:\ndisplay_images_with_bboxes(dataset_directory)","metadata":{"_uuid":"32f9fabb-75af-432f-a2f1-7ff2a10c8942","_cell_guid":"29ac5066-99fc-4cd8-ae78-2139054a25d6","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## What we know from the dataset so far\n\n- It hasnt labelled properly\n- The bounding boxes is tight, so no need readjustment\n- there's only 12 image, so might need augmentation\n- it's not square, hate it\n- and there's one image that miss one object\n\nWhat we will do next is using [Roboflow](https://roboflow.com) to adjust the label, adding augmentation and little bit of preprocessing.\n\nimo, it's ideal if we need non technical","metadata":{"_uuid":"b4370bd5-d6a9-41f4-9dac-e2d668b662ad","_cell_guid":"61216b77-02b1-4478-8e5b-fb6698378031","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Dataset augmentation, and using it.\n\nUsing Roboflow easy proprocess and augmentation, im able to convert 12 datasets image into 28(limited by Free Plan of Roboflow).\n\npreprocess also squish the image size to 320x320 pixels\n\nFor datasets this size its easy to justify using SaaS like it, for the augmentation we pick this:\n\n- Flip: Horizontal, Vertical\n- Crop: 0% Minimum Zoom, 15% Maximum Zoom\n- Rotation: Between -10° and +10°\n- Brightness: Between -15% and +15%\n- Blur: Up to 1.2px\n- Noise: Up to 0.1% of pixels\n\nThis is to represent variation of images with range of realistic error and skewness of the camera.\n\nThe datasets also automatically splitted into 3 sets:\n\n- 24 Training images\n- 3 Validation images\n- 1 Test image\n\nThe best part of using Roboflow is we can download the dataset using Curl, easy.\n\nI honestly can do that by myself with little bit of code but Roboflow able to reformat the datasets into multiple annotation formats.","metadata":{"_uuid":"7d0afe58-e936-4384-a469-a1f6ed239fe1","_cell_guid":"d991d8c1-ed4c-4cd5-aa39-27b234555c7b","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"![Roboflow Screenshot](https://github.com/dhupee/adamata-ml-engineer-test/blob/master/img/Screenshot%202025-11-24%20at%2017-01-15%20dataset-annotation%20-%20v3%202-labeled-preprocessed-augmented.png)","metadata":{"_uuid":"33b1713a-0c53-4bf8-9d4d-c7a8df81f1c8","_cell_guid":"6776127e-a374-4c09-aeef-c7d5d34c612a","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"markdown","source":"## Picking model to be used\n\nFor easy life, YOLO is probably the most straight forward model to be used, from [Models from ultralytics](https://docs.ultralytics.com/models/) docs, I decided to use Yolov11n, for smallest model of the newest Yolov11.\n\nIf given more time I might test other model even outside of Yolo, but for now Yolo11n it is.","metadata":{"_uuid":"10f3bddd-9b6d-4d68-89d6-78deed779230","_cell_guid":"d61a42f9-5c84-46d9-8b56-e45ff01d15d0","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"# this dataset isnt inside directory so we need to make new directory\nnew_dataset_url = \"https://app.roboflow.com/ds/BZ8VwtgNPT?key=yW4cr5Fxn1\"\nnew_dataset_path = \"/kaggle/working/labelled.zip\"\nnew_dataset_directory = \"labelled\"\n!curl -L {new_dataset_url} > {new_dataset_path}\n!mkdir {new_dataset_directory}\n!unzip {new_dataset_path} -d {new_dataset_directory}","metadata":{"_uuid":"e4a6ebfc-23cb-4d6d-a5ba-f8f23001b0e8","_cell_guid":"42aef525-2ea5-477b-8d3f-ed209cae15e2","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Dataset to npy conversion\n\nTHE","metadata":{"_uuid":"84dc7596-5e32-482d-9dbc-0814a6b895e9","_cell_guid":"96794571-b697-4cd7-b9c1-9174de90d2c4","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"import numpy as np\nimport cv2\nimport os\nfrom pathlib import Path\n\ndef yolo_to_npy_representative(image_dir, output_path, target_shape=(320, 320), max_samples=200):\n    \"\"\"\n    Convert multiple YOLO dataset images to NPY representative file\n    with proper error handling\n    \"\"\"\n    representative_data = []\n    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']\n    \n    # Get all image files - FIXED PATH HANDLING\n    image_files = []\n    image_dir = Path(image_dir)\n    \n    if not image_dir.exists():\n        print(f\"ERROR: Image directory '{image_dir}' does not exist!\")\n        return\n    \n    print(f\"Searching in: {image_dir.absolute()}\")\n    \n    for ext in image_extensions:\n        # Search for both lowercase and uppercase extensions\n        image_files.extend(image_dir.glob(f\"*{ext}\"))\n        image_files.extend(image_dir.glob(f\"*{ext.upper()}\"))\n    \n    # Convert to list and remove duplicates\n    image_files = list(set(image_files))\n    image_files.sort()\n    \n    print(f\"Found {len(image_files)} image files\")\n    \n    if len(image_files) == 0:\n        print(\"No images found! Check:\")\n        print(f\"1. Directory path: {image_dir.absolute()}\")\n        print(f\"2. File extensions: {image_extensions}\")\n        print(f\"3. Files actually in directory:\")\n        for file in image_dir.iterdir():\n            print(f\"   - {file.name}\")\n        return\n    \n    # Limit to max_samples\n    image_files = image_files[:max_samples]\n    \n    print(f\"Processing {len(image_files)} images...\")\n    \n    successful_count = 0\n    for i, img_path in enumerate(image_files):\n        if i % 50 == 0:\n            print(f\"Processing image {i}/{len(image_files)}...\")\n            \n        try:\n            # Load image\n            img = cv2.imread(str(img_path))\n            if img is None:\n                print(f\"  Warning: Could not load {img_path.name}\")\n                continue\n            \n            # Convert color space\n            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            \n            # Resize\n            img_resized = cv2.resize(img_rgb, target_shape)\n            \n            # Normalize (0-1 range)\n            img_normalized = img_resized.astype(np.float32) / 255.0\n            \n            representative_data.append(img_normalized)\n            successful_count += 1\n            \n        except Exception as e:\n            print(f\"  Error processing {img_path.name}: {e}\")\n            continue\n    \n    print(f\"Successfully processed {successful_count} images\")\n    \n    if successful_count == 0:\n        print(\"ERROR: No images were successfully processed!\")\n        return\n    \n    # Convert to numpy array\n    np_array = np.stack(representative_data)\n    \n    print(f\"Created representative dataset with shape: {np_array.shape}\")\n    np.save(output_path, np_array)\n    print(f\"Saved to {output_path}\")\n\n\nimage_path = \"/kaggle/working/labelled/train/images\"  # UPDATE THIS PATH\n    \nyolo_to_npy_representative(\n    image_dir=image_path,\n    output_path='representative_data.npy',\n    target_shape=(320, 320),\n    max_samples=150\n)","metadata":{"_uuid":"6f73b585-68e6-472c-9881-c1f7abd64689","_cell_guid":"9fed1709-e613-44c0-8e71-2eb08a0c1b3d","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Training the YOLO model","metadata":{"_uuid":"b457af6d-af8b-4376-871d-0452e11ddb8a","_cell_guid":"8aa4b5de-bd45-43dd-b24f-cccfc5be5515","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}}},{"cell_type":"code","source":"!pip install ultralytics","metadata":{"_uuid":"93246bf8-28e7-48f4-84f5-d7386e1f3287","_cell_guid":"cf3faaed-4ad9-4cd2-b0ad-86a95e90a104","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\nimport numpy as np\n\ndata_path = \"/kaggle/working/labelled/data.yaml\"\nimage_size = 320\nmodel = YOLO(\"yolo11n.pt\")\nresults = model.train(data=data_path, epochs=100, imgsz=image_size)","metadata":{"_uuid":"8961411a-cf16-4274-9203-ae1a96d582e9","_cell_guid":"cf57b959-65d9-4639-95b7-b32000fa5729","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display training metrics\nprint(\"\\n=== Training Metrics Summary ===\\n\")\n\n# Get the latest training results\nfrom ultralytics.utils.metrics import ConfusionMatrix\nimport pandas as pd\n\n# Load the results from the last training run\ntry:\n    # Get the path to the latest training run\n    import glob\n    latest_run = sorted(glob.glob('/kaggle/working/runs/detect/train*'))[-1]\n    results_path = f\"{latest_run}/results.csv\"\n    \n    # Read and display the results\n    results_df = pd.read_csv(results_path)\n    print(\"Last 5 epochs metrics:\")\n    print(results_df.tail().to_string(index=False))\n    \n    # Show key final metrics\n    final_metrics = results_df.iloc[-1]\n    print(f\"\\n=== Final Epoch Metrics ===\")\n    print(f\"Box Loss: {final_metrics['train/box_loss']:.4f}\")\n    print(f\"Class Loss: {final_metrics['train/cls_loss']:.4f}\")\n    print(f\"DFL Loss: {final_metrics['train/dfl_loss']:.4f}\")\n    print(f\"Precision: {final_metrics['metrics/precision(B)']:.4f}\")\n    print(f\"Recall: {final_metrics['metrics/recall(B)']:.4f}\")\n    print(f\"mAP@0.5: {final_metrics['metrics/mAP50(B)']:.4f}\")\n    print(f\"mAP@0.5:0.95: {final_metrics['metrics/mAP50-95(B)']:.4f}\")\n    \nexcept Exception as e:\n    print(f\"Error loading metrics: {e}\")\n    print(\"Showing available metrics from training results...\")\n    # Fallback to simple metrics display\n    if 'results' in locals():\n        print(\"Training completed. Check the runs/detect/train directory for detailed results.\")","metadata":{"_uuid":"4e15cf08-e518-407a-ac0d-0824e3dcc90a","_cell_guid":"09fadb2e-4ea1-4381-9b17-169c731e8cec","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Validate the model and show validation metrics\nprint(\"\\n=== Validation Metrics ===\\n\")\n\n# Run validation on the model\nval_metrics = model.val()","metadata":{"_uuid":"189c76f8-5c12-4b1b-a9c2-64639027ee87","_cell_guid":"8305d842-ab9e-42a5-aaa5-fb91e861e8c5","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export the trained model\nprint(\"\\n=== Exporting Model ===\\n\")\n\n# Export to different formats\nexport_formats = ['onnx', 'torchscript']  # You can add more formats if needed\n\nfor fmt in export_formats:\n    try:\n        export_path = model.export(format=fmt)\n        print(f\"✓ Model exported to {fmt.upper()} format: {export_path}\")\n    except Exception as e:\n        print(f\"✗ Failed to export to {fmt.upper()}: {e}\")\n\n# Also save as PyTorch model\nmodel_path = \"/kaggle/working/adamata_trained_model.pt\"\nmodel.save(model_path)\nprint(f\"✓ Model saved as PyTorch: {model_path}\")\n\n# List exported files\nprint(f\"\\n=== Exported Files ===\")\n!find /kaggle/working -name \"*.pt\" -o -name \"*.onnx\" -o -name \"*.torchscript\" | grep -v \"__pycache__\"","metadata":{"_uuid":"a8af2048-26c2-490d-9710-2cf9896e4382","_cell_guid":"219b3c1e-35c4-42f9-bb78-6c30204eb993","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Simple test inference on a specific exported model\nprint(\"\\n=== Testing Exported Model ===\\n\")\n\n# Load the exported model\nmodel_path = \"/kaggle/working/runs/detect/train/weights/best.onnx\"  # Change this path if needed\ntest_model = YOLO(model_path)\nprint(f\"Loaded model: {model_path}\")\n\nimport glob\n# Find a test image\ntest_images = glob.glob(f\"{new_dataset_directory}/test/images/*.jpg\")\nif test_images:\n    test_image = test_images[0]\n    print(f\"Testing on: {test_image}\")\n    \n    # Run inference\n    results = test_model.predict(test_image, conf=0.5)\n    \n    # Show results\n    for i, r in enumerate(results):\n        print(f\"\\nImage {i+1}:\")\n        print(f\"Detected {len(r.boxes)} objects\")\n        if len(r.boxes) > 0:\n            print(\"Detections:\")\n            for j, box in enumerate(r.boxes):\n                cls = int(box.cls[0])\n                conf = float(box.conf[0])\n                print(f\"  Object {j+1}: Class {cls}, Confidence: {conf:.4f}\")\nelse:\n    print(\"No test images found for inference demo\")","metadata":{"_uuid":"07529eca-5005-416e-9ba7-a87835bbd7e5","_cell_guid":"23bf0dd0-d875-41a4-a9c1-c6ff980746a9","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display model architecture summary\nprint(\"\\n=== Model Architecture Summary ===\\n\")\nprint(f\"Model: {model.__class__.__name__}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")\nprint(f\"Number of layers: {len(list(model.model.modules()))}\")\n\n# Show class names\nif hasattr(model, 'names') and model.names:\n    print(f\"\\nClass names: {model.names}\")","metadata":{"_uuid":"42c636c4-bc84-49c1-bfb1-6efbe3ab9a0a","_cell_guid":"c957c96e-154f-4367-8362-a82f3768f26c","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip freeze","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}